{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from irt import *\n",
    "from utils import *\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "model_list = [\n",
    "    'microsoft/Phi-3-medium-4k-instruct',\n",
    "    'internlm/internlm2_5-7b-chat',\n",
    "    'microsoft/Phi-3-small-128k-instruct',\n",
    "    '01-ai/Yi-1.5-9B-Chat',\n",
    "    'MaziyarPanahi/Llama-3-8B-Instruct-v0.8',\n",
    "    'Qwen/Qwen2-7B-Instruct',\n",
    "    'NousResearch/Hermes-2-Theta-Llama-3-8B',\n",
    "    'vicgalle/Roleplay-Llama-3-8B',\n",
    "    'Qwen/Qwen2-7B',\n",
    "    'NousResearch/Nous-Hermes-2-SOLAR-10.7B',\n",
    "    'UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter3',\n",
    "    '01-ai/Yi-1.5-9B-Chat-16K',\n",
    "    'refuelai/Llama-3-Refueled',\n",
    "    'openchat/openchat-3.6-8b-20240522',\n",
    "    'openchat/openchat-3.5-0106',\n",
    "    'openchat/openchat-3.5-1210',\n",
    "    '01-ai/Yi-1.5-6B-Chat',\n",
    "    'mlabonne/NeuralDaredevil-8B-abliterated',\n",
    "    '01-ai/Yi-1.5-9B',\n",
    "    'NousResearch/Hermes-2-Pro-Mistral-7B',\n",
    "    'NousResearch/Hermes-2-Pro-Llama-3-8B',\n",
    "    'openchat/openchat_3.5',\n",
    "    'Intel/neural-chat-7b-v3-2',\n",
    "    'teknium/OpenHermes-2-Mistral-7B',\n",
    "    'teknium/OpenHermes-2.5-Mistral-7B',\n",
    "    'NousResearch/Nous-Hermes-2-Mistral-7B-DPO',\n",
    "    'Intel/neural-chat-7b-v3-1',\n",
    "    'berkeley-nest/Starling-LM-7B-alpha',\n",
    "    'Intel/neural-chat-7b-v3-3',\n",
    "    'upstage/SOLAR-10.7B-Instruct-v1.0',\n",
    "    # more\n",
    "    'RLHFlow/LLaMA3-iterative-DPO-final',\n",
    "    '01-ai/Yi-1.5-9B-32K',\n",
    "    'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    'HuggingFaceH4/zephyr-7b-alpha',\n",
    "    'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "    'cognitivecomputations/dolphin-2.9-llama3-8b',\n",
    "    'meta-llama/Llama-2-70b-hf',\n",
    "    'microsoft/Orca-2-13b',\n",
    "    'gradientai/Llama-3-8B-Instruct-Gradient-1048k',\n",
    "    'THUDM/glm-4-9b',\n",
    "    'Intel/neural-chat-7b-v3',\n",
    "    'HuggingFaceH4/zephyr-7b-beta',\n",
    "    'Open-Orca/Mistral-7B-OpenOrca',\n",
    "    '01-ai/Yi-9B',\n",
    "    '01-ai/Yi-9B-200K',\n",
    "    'Deci/DeciLM-7B-instruct',\n",
    "    'google/gemma-1.1-7b-it',\n",
    "    'upstage/SOLAR-10.7B-v1.0',\n",
    "    'ibm/merlinite-7b',\n",
    "    'LLM360/CrystalChat',\n",
    "    '01-ai/Yi-1.5-6B',\n",
    "    'stabilityai/stablelm-2-12b-chat',\n",
    "    'CohereForAI/aya-23-8B',\n",
    "    'HuggingFaceH4/zephyr-7b-gemma-v0.1',\n",
    "    'NousResearch/Yarn-Solar-10b-32k',\n",
    "    'microsoft/phi-2',\n",
    "    'google/gemma-7b',\n",
    "    'Qwen/Qwen1.5-7B',\n",
    "    'WizardLMTeam/WizardLM-13B-V1.2',\n",
    "    'TencentARC/LLaMA-Pro-8B-Instruct',\n",
    "    'NousResearch/Yarn-Solar-10b-64k',\n",
    "    'Deci/DeciLM-7B',\n",
    "    'mlabonne/OrpoLlama-3-8B',\n",
    "    'deepseek-ai/deepseek-llm-7b-chat',\n",
    "    'mistralai/Mistral-7B-v0.1',\n",
    "    'teknium/CollectiveCognition-v1.1-Mistral-7B',\n",
    "    'TencentARC/Mistral_Pro_8B_v0.1',\n",
    "    'mistralai/Mistral-7B-v0.3',\n",
    "    'microsoft/Orca-2-7b',\n",
    "    'mistral-community/Mistral-7B-v0.2',\n",
    "    '01-ai/Yi-6B-Chat',\n",
    "    'Qwen/Qwen2-1.5B-Instruct',\n",
    "    'stabilityai/stablelm-2-12b',\n",
    "    'openchat/openchat_v3.2',\n",
    "    'tiiuae/falcon-11B',\n",
    "    '01-ai/Yi-6B',\n",
    "    'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "    'NousResearch/Yarn-Mistral-7b-64k',\n",
    "]\n",
    "\n",
    "log_dir = '/data/amos_zhu/lm-evaluation-harness/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['anchor_points', 'anchor_weights'])\n"
     ]
    }
   ],
   "source": [
    "# read pkl\n",
    "with open(f'data/anchor.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n{'correctness': array[q_num][model_num])\\n}\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios = {'wmdp': ['wmdp_cyber', 'wmdp_bio']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_data(model_dir):\n",
    "    log_names = os.listdir(model_dir)\n",
    "    wmdp_bio_log = None\n",
    "    wmdp_cyber_log = None\n",
    "    results_log = None\n",
    "    for log_name in log_names:\n",
    "        if 'wmdp_bio' in log_name:\n",
    "            wmdp_bio_log_dir = os.path.join(model_dir, log_name)\n",
    "            with open(wmdp_bio_log_dir, 'r') as f:\n",
    "                wmdp_bio_log = []\n",
    "                for line in f:\n",
    "                    json_data = json.loads(line.strip())\n",
    "                    wmdp_bio_log.append(json_data)\n",
    "            \n",
    "                \n",
    "        elif 'wmdp_cyber' in log_name:\n",
    "            wmdp_cyber_log_dir = os.path.join(model_dir, log_name)\n",
    "            with open(wmdp_cyber_log_dir, 'r') as f:\n",
    "                wmdp_cyber_log = []\n",
    "                for line in f:\n",
    "                    json_data = json.loads(line.strip())\n",
    "                    wmdp_cyber_log.append(json_data)\n",
    "        elif 'results' in log_name:\n",
    "            results_log_dir = os.path.join(model_dir, log_name)\n",
    "            with open(results_log_dir, 'r') as f:\n",
    "                results_log = json.load(f)\n",
    "    \n",
    "    return wmdp_bio_log, wmdp_cyber_log, results_log\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "wmdp_bio_logs = []\n",
    "wmdp_cyber_logs = []\n",
    "results_logs = []\n",
    "model_names = []\n",
    "\n",
    "import os\n",
    "for model_name in model_list:\n",
    "    model_name = model_name.split('/')\n",
    "    org_dir = os.path.join(log_dir, model_name[0], model_name[1])\n",
    "    full_model_name = '__'.join(model_name)\n",
    "    model_dir = os.path.join(org_dir, full_model_name)\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        # print(f'{full_model_name} not found')\n",
    "        # failed_model_num += 1\n",
    "        continue\n",
    "\n",
    "    wmdp_bio_log, wmdp_cyber_log, results_log = extract_data(model_dir)\n",
    "    wmdp_bio_logs.append(wmdp_bio_log)\n",
    "    wmdp_cyber_logs.append(wmdp_cyber_log)\n",
    "    results_logs.append(results_log)\n",
    "    model_names.append(full_model_name.replace('__', '/'))\n",
    "\n",
    "# print(f'{failed_model_num} out of {total_model_num} models not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_models: 64, num_questions: 1273\n",
      "num_models: 64, num_questions: 1987\n",
      "Saved to /data/amos_zhu/tinyBenchmarks/tutorials/wmdp_data.pkl\n"
     ]
    }
   ],
   "source": [
    "def process_wmdp_logs(wmdp_logs):\n",
    "    '''\n",
    "    {'correctness': array[q_num][model_num])\n",
    "    }\n",
    "    '''\n",
    "    num_models = len(wmdp_logs)\n",
    "    num_questions = len(wmdp_logs[0])\n",
    "    print(f'num_models: {num_models}, num_questions: {num_questions}')\n",
    "    correctness = np.zeros((num_questions, num_models))\n",
    "    for i, wmdp_log in enumerate(wmdp_logs):\n",
    "        wmdp_log.sort(key=lambda x: x['doc_id'])\n",
    "        for j, log in enumerate(wmdp_log):\n",
    "            q_index = int(log['doc_id'])\n",
    "            model_index = i\n",
    "            correctness[q_index, model_index] = log['acc']\n",
    "    return {'correctness': correctness}\n",
    "\n",
    "def process_logs(wmdp_bio_logs,\n",
    "                 wmdp_cyber_logs,\n",
    "                 results_logs,\n",
    "                 model_names):\n",
    "    data = {'models': [], 'data': {}}  \n",
    "\n",
    "    data['models'] = model_names\n",
    "\n",
    "    bio_matrix = process_wmdp_logs(wmdp_bio_logs)\n",
    "    cyber_matrix = process_wmdp_logs(wmdp_cyber_logs)\n",
    "    data['data']['wmdp_bio'] = bio_matrix\n",
    "    data['data']['wmdp_cyber'] = cyber_matrix\n",
    "\n",
    "    # save to pickle\n",
    "    output_dir = '/data/amos_zhu/tinyBenchmarks/tutorials'\n",
    "    output_dir = os.path.join(output_dir, f'wmdp_data.pkl')\n",
    "    with open(output_dir, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f'Saved to {output_dir}')\n",
    "\n",
    "process_logs(wmdp_bio_logs, wmdp_cyber_logs, results_logs, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['microsoft/Phi-3-medium-4k-instruct', 'internlm/internlm2_5-7b-chat', '01-ai/Yi-1.5-9B-Chat', 'MaziyarPanahi/Llama-3-8B-Instruct-v0.8', 'Qwen/Qwen2-7B-Instruct', 'NousResearch/Hermes-2-Theta-Llama-3-8B', 'vicgalle/Roleplay-Llama-3-8B', 'Qwen/Qwen2-7B', 'NousResearch/Nous-Hermes-2-SOLAR-10.7B', 'UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter3', '01-ai/Yi-1.5-9B-Chat-16K', 'refuelai/Llama-3-Refueled', 'openchat/openchat-3.6-8b-20240522', 'openchat/openchat-3.5-0106', 'openchat/openchat-3.5-1210', '01-ai/Yi-1.5-6B-Chat', 'mlabonne/NeuralDaredevil-8B-abliterated', '01-ai/Yi-1.5-9B', 'NousResearch/Hermes-2-Pro-Mistral-7B', 'NousResearch/Hermes-2-Pro-Llama-3-8B', 'openchat/openchat_3.5', 'Intel/neural-chat-7b-v3-2', 'teknium/OpenHermes-2-Mistral-7B', 'teknium/OpenHermes-2.5-Mistral-7B', 'NousResearch/Nous-Hermes-2-Mistral-7B-DPO', 'Intel/neural-chat-7b-v3-1', 'berkeley-nest/Starling-LM-7B-alpha', 'Intel/neural-chat-7b-v3-3', 'upstage/SOLAR-10.7B-Instruct-v1.0', 'RLHFlow/LLaMA3-iterative-DPO-final', '01-ai/Yi-1.5-9B-32K', 'HuggingFaceH4/zephyr-7b-alpha', 'gradientai/Llama-3-8B-Instruct-Gradient-1048k', 'THUDM/glm-4-9b', 'Intel/neural-chat-7b-v3', 'HuggingFaceH4/zephyr-7b-beta', 'Open-Orca/Mistral-7B-OpenOrca', '01-ai/Yi-9B', '01-ai/Yi-9B-200K', 'Deci/DeciLM-7B-instruct', 'upstage/SOLAR-10.7B-v1.0', 'ibm/merlinite-7b', 'LLM360/CrystalChat', '01-ai/Yi-1.5-6B', 'stabilityai/stablelm-2-12b-chat', 'HuggingFaceH4/zephyr-7b-gemma-v0.1', 'NousResearch/Yarn-Solar-10b-32k', 'microsoft/phi-2', 'Qwen/Qwen1.5-7B', 'WizardLMTeam/WizardLM-13B-V1.2', 'TencentARC/LLaMA-Pro-8B-Instruct', 'NousResearch/Yarn-Solar-10b-64k', 'mlabonne/OrpoLlama-3-8B', 'deepseek-ai/deepseek-llm-7b-chat', 'teknium/CollectiveCognition-v1.1-Mistral-7B', 'TencentARC/Mistral_Pro_8B_v0.1', 'microsoft/Orca-2-7b', 'mistral-community/Mistral-7B-v0.2', '01-ai/Yi-6B-Chat', 'Qwen/Qwen2-1.5B-Instruct', 'stabilityai/stablelm-2-12b', 'openchat/openchat_v3.2', '01-ai/Yi-6B', 'NousResearch/Yarn-Mistral-7b-64k']\n",
      "(1273, 64)\n",
      "(1987, 64)\n"
     ]
    }
   ],
   "source": [
    "# try load\n",
    "output_dir = '/data/amos_zhu/tinyBenchmarks/tutorials'\n",
    "output_dir = os.path.join(output_dir, f'wmdp_data.pkl')\n",
    "with open(output_dir, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    print(data['models'])\n",
    "    print(data['data']['wmdp_bio']['correctness'].shape)\n",
    "    print(data['data']['wmdp_cyber']['correctness'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinybenchmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
