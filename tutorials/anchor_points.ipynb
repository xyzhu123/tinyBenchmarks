{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb0a714-9d15-41bc-ac06-ce5a1e448b1d",
   "metadata": {},
   "source": [
    "# Finding and using anchor points\n",
    "\n",
    "In this notebook, we show how to find anchor points based on your training set and how to use them to estimate the performance of new models in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008d96d-1ee5-44cf-91cb-293fb3e048bf",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d9b93-5059-416c-8a53-e3b4cc24a904",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7892164d-f5bb-4cef-9f4f-685a9af85679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from irt import *\n",
    "from utils import *\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb8ce2-1851-4131-8d35-36214be71085",
   "metadata": {},
   "source": [
    "The leaderboard dataset we will use is composed by six scenarios (sub-datasets):\n",
    "1. TruthfulQA\n",
    "1. GSM8K\n",
    "1. Winogrande\n",
    "1. ARC\n",
    "1. HellaSwag\n",
    "1. MMLU\n",
    "\n",
    "MMLU is further divided into sub-scenarios (e.g., abstract algebra, anatomy, etc). Let's check scenarios and sub-scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26499fc1-2bda-44b2-9131-e78d16f7f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios\n",
    "scenarios = {'wmdp_cyber':['wmdp_cyber'],\n",
    "             'wmdp_bio':['wmdp_bio'],}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e5620-bd45-4985-b390-a154843b4d6c",
   "metadata": {},
   "source": [
    "Loading leaderboard data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ca68f5c-49de-4f75-92e5-de639059cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wmdp_data.pkl', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14f180-d322-4cd5-8d0c-4ae4fef04127",
   "metadata": {},
   "source": [
    "In this dataset, we have data from 395 models. Let's see the names of some of them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d6c4201-0675-42e5-8a7a-8cf75592e661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,\n",
       " ['microsoft/Phi-3-medium-4k-instruct',\n",
       "  'internlm/internlm2_5-7b-chat',\n",
       "  '01-ai/Yi-1.5-9B-Chat',\n",
       "  'MaziyarPanahi/Llama-3-8B-Instruct-v0.8',\n",
       "  'Qwen/Qwen2-7B-Instruct',\n",
       "  'NousResearch/Hermes-2-Theta-Llama-3-8B',\n",
       "  'vicgalle/Roleplay-Llama-3-8B',\n",
       "  'Qwen/Qwen2-7B',\n",
       "  'NousResearch/Nous-Hermes-2-SOLAR-10.7B',\n",
       "  'UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter3'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['models']),data['models'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8135b-e9ec-468a-85a7-3cd3fc3d31fe",
   "metadata": {},
   "source": [
    "Below, we will process the data so all correctness scores (for all scenarios) are stored in $Y$. The dictionaries `scenarios_position` and `subscenarios_position` give the position of scenarios/subscenarios correctness scores in $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee09c25b-2dc4-4403-a972-9fb05cfe917b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3260)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios_position, subscenarios_position = prepare_data(scenarios, data)\n",
    "Y = create_responses(scenarios, data)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002485a-1e82-409b-aaf2-ddb6a82bc315",
   "metadata": {},
   "source": [
    "For example, below you can see the scores for MMLU:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662681a7-10b0-4ddc-a692-52d278499539",
   "metadata": {},
   "source": [
    "For scenarios that have multiple subscenarios, it is usually the case that we want to give equal importance to individual subscenarios when computing the aggregated performance in that scenario. This is equivalent to using a weighted average when computing the aggregated performance. We will create `balance_weights`, a vector of weights to help us compute those weighted averages. These weights will be different than one only for MMLU, which is the only scenario with multiple subscenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f40fc53-b11e-41cc-adc2-7abff1a2b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_weights = np.ones(Y.shape[1])\n",
    "\n",
    "# N = len(scenarios_position['mmlu'])\n",
    "# n_sub = len(scenarios['mmlu'])\n",
    "# for sub in scenarios['mmlu']:\n",
    "#     n_i = len(subscenarios_position['mmlu'][sub])\n",
    "#     balance_weights[subscenarios_position['mmlu'][sub]] = N/(n_sub*n_i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d42b7-c6ac-4695-a5f0-3087c091d16d",
   "metadata": {},
   "source": [
    "We can see below that first averaging within subscenarios and then computing a simple average is equivalent to using a weighted average from the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b51b6f-5ce5-46bf-ba44-836386db05f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.322333605307685e-14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accs1 = np.mean([Y[:,subscenarios_position['mmlu'][sub]].mean(axis=1) for sub in scenarios['mmlu']], axis=0)\n",
    "# accs2 = (balance_weights*Y)[:,scenarios_position['mmlu']].mean(axis=1)\n",
    "\n",
    "# np.abs(accs1 - accs2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106b620-7fe0-49bb-a8ac-3a946c15f751",
   "metadata": {},
   "source": [
    "## Getting and using anchor points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c4412-ae69-4184-b106-191a1c151736",
   "metadata": {},
   "source": [
    "Let's split the data in train and test (recent models are placed in the test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc9874c5-7cb5-425b-8c41-9a87d7615ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y[:16]\n",
    "Y_train = Y[16:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680d6e6-1ec2-4a24-a898-f29bd5ec109e",
   "metadata": {},
   "source": [
    "The variable `number_item` gives the number of anchor points we want to find in each scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4a80e3b-8e0c-402f-8263-7e178b976bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_item = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528f89a-64bb-497e-b993-9181996d75d1",
   "metadata": {},
   "source": [
    "The variable `clustering` specified how the clusting is run. If `clustering=\"correct.\"`, then correctness is used. On the other hand, if `clustering=\"irt\"`, then the IRT embeddings for examples are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5860e70f-9ebb-4181-bd14-276bb5057ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = 'irt' # 'correct.' or 'irt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5edf1d-21b3-46f9-9034-479ebe89314d",
   "metadata": {},
   "source": [
    "Computing anchor points and their weights for each scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "313c85b8-838d-416c-ac7d-e40725e08853",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_points = {}\n",
    "anchor_weights = {}\n",
    "\n",
    "for scenario in scenarios.keys():\n",
    "\n",
    "    if clustering=='correct.':\n",
    "        X = Y_train[:,scenarios_position[scenario]].T\n",
    "    elif clustering=='irt':\n",
    "        A, B, _ = load_irt_parameters('data/irt_model/')\n",
    "        X = np.vstack((A.squeeze(), B.squeeze().reshape((1,-1)))).T\n",
    "        X = X[scenarios_position[scenario]]\n",
    "    else:\n",
    "        raise NotImplementedError \n",
    "        \n",
    "    #Normalizing balance_weights, so their sum is one within each scenario\n",
    "    norm_balance_weights = balance_weights[scenarios_position[scenario]]\n",
    "    norm_balance_weights /= norm_balance_weights.sum()\n",
    "\n",
    "    # Fitting the KMeans model\n",
    "    kmeans = KMeans(n_clusters=number_item, n_init=\"auto\", random_state=random_state)\n",
    "    kmeans.fit(X, sample_weight=norm_balance_weights)\n",
    "\n",
    "    # Calculating anchor points\n",
    "    anchor_points[scenario] = pairwise_distances(kmeans.cluster_centers_, X, metric='euclidean').argmin(axis=1)\n",
    "\n",
    "    # Calculating anchor weights\n",
    "    anchor_weights[scenario] = np.array([np.sum(norm_balance_weights[kmeans.labels_==c]) for c in range(number_item)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c071b44-1410-4cb8-9d20-2f5a3ed9c5c9",
   "metadata": {},
   "source": [
    "Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11008304-b6db-4b55-863d-c9968a0b76ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = {'anchor_points':anchor_points,\n",
    "          'anchor_weights':anchor_weights}\n",
    "\n",
    "with open('data/anchor.pickle', 'wb') as handle:\n",
    "    pickle.dump(anchor, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651edaf8-af28-4e6f-92d6-192477c0aa44",
   "metadata": {},
   "source": [
    "Checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a468b8f-950b-41c5-b009-e9b3d913b0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1126,  201,  136, 1851, 1159, 1175, 1630,  425,  777, 1710, 1661,\n",
       "       1129,  473,  399, 1442, 1312, 1689, 1745,  696, 1101,  831, 1494,\n",
       "        683,  480,  500,   65, 1097, 1341,  300, 1400,  330,  454, 1950,\n",
       "       1751, 1761,  591,  345,  254, 1003, 1037,  999,  735, 1463,  750,\n",
       "        987,  638,   11,  353, 1540,   85, 1533, 1868, 1420,  126,  716,\n",
       "       1680,  191,  563,  403, 1046, 1802, 1235, 1546,  682,  600,  827,\n",
       "       1142, 1545, 1176,  782, 1106,  261, 1396, 1554, 1819,  451, 1252,\n",
       "          7, 1052, 1030, 1864,  429,  173,  780, 1196, 1156, 1757,  784,\n",
       "       1862,  954,  408,  669,  601, 1916, 1604, 1443,  986, 1529, 1361,\n",
       "        263])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_points['wmdp_cyber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6955cb1-de2d-4346-a12e-4356e464c2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01207851, 0.01258178, 0.0140916 , 0.00905888, 0.01157524,\n",
       "       0.00603926, 0.02365375, 0.00905888, 0.00754907, 0.01258178,\n",
       "       0.00553598, 0.00503271, 0.00905888, 0.0140916 , 0.00855561,\n",
       "       0.0105687 , 0.00855561, 0.00805234, 0.01610468, 0.00855561,\n",
       "       0.01358832, 0.01157524, 0.00654253, 0.00855561, 0.00855561,\n",
       "       0.00855561, 0.00754907, 0.00452944, 0.00855561, 0.01006543,\n",
       "       0.0035229 , 0.0070458 , 0.00754907, 0.00805234, 0.01006543,\n",
       "       0.01711122, 0.01509814, 0.00956215, 0.0070458 , 0.01258178,\n",
       "       0.01107197, 0.00603926, 0.02013085, 0.0105687 , 0.01207851,\n",
       "       0.00603926, 0.01006543, 0.01006543, 0.02717665, 0.00654253,\n",
       "       0.00905888, 0.00855561, 0.05485657, 0.01157524, 0.00402617,\n",
       "       0.00805234, 0.01509814, 0.00855561, 0.0140916 , 0.00654253,\n",
       "       0.00855561, 0.00956215, 0.00805234, 0.00905888, 0.01207851,\n",
       "       0.00805234, 0.00503271, 0.00754907, 0.00452944, 0.00553598,\n",
       "       0.00503271, 0.00402617, 0.00603926, 0.00805234, 0.01006543,\n",
       "       0.00905888, 0.00905888, 0.00905888, 0.00855561, 0.0070458 ,\n",
       "       0.00553598, 0.00603926, 0.00654253, 0.00603926, 0.00805234,\n",
       "       0.00654253, 0.00905888, 0.0035229 , 0.00905888, 0.00553598,\n",
       "       0.00251636, 0.00603926, 0.00805234, 0.00754907, 0.00603926,\n",
       "       0.00855561, 0.01358832, 0.01459487, 0.0070458 , 0.04781077])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_weights['wmdp_cyber']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac7f2a-9288-4ef5-aa57-7280523cfd4c",
   "metadata": {},
   "source": [
    "Using anchor points to estimate performance in the test set and reporting the average prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a79301f-1140-474e-bd41-832f9c6d0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario: wmdp_cyber, avg. error: 0.024, std. dev.: 0.015\n",
      "scenario: wmdp_bio, avg. error: 0.024, std. dev.: 0.018\n"
     ]
    }
   ],
   "source": [
    "for scenario in scenarios.keys():\n",
    "    Y_anchor = Y_test[:,scenarios_position[scenario]][:,anchor_points[scenario]]\n",
    "    Y_hat = (Y_anchor*anchor_weights[scenario]).sum(axis=1)\n",
    "    Y_true = (balance_weights*Y_test)[:,scenarios_position[scenario]].mean(axis=1)\n",
    "    avg_error = np.abs(Y_hat-Y_true).mean()\n",
    "    sd = np.abs(Y_hat-Y_true).std()\n",
    "\n",
    "    print(f\"scenario: {scenario}, avg. error: {avg_error:.3f}, std. dev.: {sd:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
