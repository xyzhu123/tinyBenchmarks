{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb0a714-9d15-41bc-ac06-ce5a1e448b1d",
   "metadata": {},
   "source": [
    "# Estimating performance\n",
    "\n",
    "In this notebook, we show how to obtain estimates for LLM performance by combining anchor points and IRT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008d96d-1ee5-44cf-91cb-293fb3e048bf",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d9b93-5059-416c-8a53-e3b4cc24a904",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7892164d-f5bb-4cef-9f4f-685a9af85679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from irt import *\n",
    "from utils import *\n",
    "\n",
    "random_state = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb8ce2-1851-4131-8d35-36214be71085",
   "metadata": {},
   "source": [
    "The leaderboard dataset we will use is composed by six scenarios (sub-datasets):\n",
    "1. TruthfulQA\n",
    "1. GSM8K\n",
    "1. Winogrande\n",
    "1. ARC\n",
    "1. HellaSwag\n",
    "1. MMLU\n",
    "\n",
    "MMLU is further divided into sub-scenarios (e.g., abstract algebra, anatomy, etc). Let's check scenarios and sub-scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26499fc1-2bda-44b2-9131-e78d16f7f77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'harness_truthfulqa_mc_0': ['harness_truthfulqa_mc_0'],\n",
       " 'gsm8k': ['harness_gsm8k_5'],\n",
       " 'winogrande': ['harness_winogrande_5'],\n",
       " 'arc': ['harness_arc_challenge_25'],\n",
       " 'hellaswag': ['harness_hellaswag_10'],\n",
       " 'mmlu': ['harness_hendrycksTest_abstract_algebra_5',\n",
       "  'harness_hendrycksTest_anatomy_5',\n",
       "  'harness_hendrycksTest_astronomy_5',\n",
       "  'harness_hendrycksTest_business_ethics_5',\n",
       "  'harness_hendrycksTest_clinical_knowledge_5',\n",
       "  'harness_hendrycksTest_college_biology_5',\n",
       "  'harness_hendrycksTest_college_chemistry_5',\n",
       "  'harness_hendrycksTest_college_computer_science_5',\n",
       "  'harness_hendrycksTest_college_mathematics_5',\n",
       "  'harness_hendrycksTest_college_medicine_5',\n",
       "  'harness_hendrycksTest_college_physics_5',\n",
       "  'harness_hendrycksTest_computer_security_5',\n",
       "  'harness_hendrycksTest_conceptual_physics_5',\n",
       "  'harness_hendrycksTest_econometrics_5',\n",
       "  'harness_hendrycksTest_electrical_engineering_5',\n",
       "  'harness_hendrycksTest_elementary_mathematics_5',\n",
       "  'harness_hendrycksTest_formal_logic_5',\n",
       "  'harness_hendrycksTest_global_facts_5',\n",
       "  'harness_hendrycksTest_high_school_biology_5',\n",
       "  'harness_hendrycksTest_high_school_chemistry_5',\n",
       "  'harness_hendrycksTest_high_school_computer_science_5',\n",
       "  'harness_hendrycksTest_high_school_european_history_5',\n",
       "  'harness_hendrycksTest_high_school_geography_5',\n",
       "  'harness_hendrycksTest_high_school_government_and_politics_5',\n",
       "  'harness_hendrycksTest_high_school_macroeconomics_5',\n",
       "  'harness_hendrycksTest_high_school_mathematics_5',\n",
       "  'harness_hendrycksTest_high_school_microeconomics_5',\n",
       "  'harness_hendrycksTest_high_school_physics_5',\n",
       "  'harness_hendrycksTest_high_school_psychology_5',\n",
       "  'harness_hendrycksTest_high_school_statistics_5',\n",
       "  'harness_hendrycksTest_high_school_us_history_5',\n",
       "  'harness_hendrycksTest_high_school_world_history_5',\n",
       "  'harness_hendrycksTest_human_aging_5',\n",
       "  'harness_hendrycksTest_human_sexuality_5',\n",
       "  'harness_hendrycksTest_international_law_5',\n",
       "  'harness_hendrycksTest_jurisprudence_5',\n",
       "  'harness_hendrycksTest_logical_fallacies_5',\n",
       "  'harness_hendrycksTest_machine_learning_5',\n",
       "  'harness_hendrycksTest_management_5',\n",
       "  'harness_hendrycksTest_marketing_5',\n",
       "  'harness_hendrycksTest_medical_genetics_5',\n",
       "  'harness_hendrycksTest_miscellaneous_5',\n",
       "  'harness_hendrycksTest_moral_disputes_5',\n",
       "  'harness_hendrycksTest_moral_scenarios_5',\n",
       "  'harness_hendrycksTest_nutrition_5',\n",
       "  'harness_hendrycksTest_philosophy_5',\n",
       "  'harness_hendrycksTest_prehistory_5',\n",
       "  'harness_hendrycksTest_professional_accounting_5',\n",
       "  'harness_hendrycksTest_professional_law_5',\n",
       "  'harness_hendrycksTest_professional_medicine_5',\n",
       "  'harness_hendrycksTest_professional_psychology_5',\n",
       "  'harness_hendrycksTest_public_relations_5',\n",
       "  'harness_hendrycksTest_security_studies_5',\n",
       "  'harness_hendrycksTest_sociology_5',\n",
       "  'harness_hendrycksTest_us_foreign_policy_5',\n",
       "  'harness_hendrycksTest_virology_5',\n",
       "  'harness_hendrycksTest_world_religions_5']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e5620-bd45-4985-b390-a154843b4d6c",
   "metadata": {},
   "source": [
    "Loading leaderboard data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca68f5c-49de-4f75-92e5-de639059cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lb.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c9eb650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395\n",
      "(100, 395)\n",
      "(135, 395)\n",
      "(152, 395)\n",
      "(100, 395)\n",
      "(265, 395)\n",
      "(144, 395)\n",
      "(100, 395)\n",
      "(100, 395)\n",
      "(100, 395)\n",
      "(173, 395)\n",
      "(102, 395)\n",
      "(100, 395)\n",
      "(235, 395)\n",
      "(114, 395)\n",
      "(145, 395)\n",
      "(378, 395)\n",
      "(126, 395)\n",
      "(100, 395)\n",
      "(310, 395)\n",
      "(203, 395)\n",
      "(100, 395)\n",
      "(165, 395)\n",
      "(198, 395)\n",
      "(193, 395)\n",
      "(390, 395)\n",
      "(270, 395)\n",
      "(238, 395)\n",
      "(151, 395)\n",
      "(545, 395)\n",
      "(216, 395)\n",
      "(204, 395)\n",
      "(237, 395)\n",
      "(223, 395)\n",
      "(131, 395)\n",
      "(121, 395)\n",
      "(108, 395)\n",
      "(163, 395)\n",
      "(112, 395)\n",
      "(103, 395)\n",
      "(234, 395)\n",
      "(100, 395)\n",
      "(783, 395)\n",
      "(346, 395)\n",
      "(895, 395)\n",
      "(306, 395)\n",
      "(311, 395)\n",
      "(324, 395)\n",
      "(282, 395)\n",
      "(1534, 395)\n",
      "(272, 395)\n",
      "(612, 395)\n",
      "(110, 395)\n",
      "(245, 395)\n",
      "(201, 395)\n",
      "(100, 395)\n",
      "(166, 395)\n",
      "(171, 395)\n",
      "(1172, 395)\n",
      "(10042, 395)\n",
      "(817, 395)\n",
      "(1267, 395)\n",
      "(1319, 395)\n",
      "(100, 395)\n"
     ]
    }
   ],
   "source": [
    "print(len(data['models']))\n",
    "c_data = data['data']\n",
    "categories = list(c_data.keys())\n",
    "for c in categories:\n",
    "    print(c_data[c]['correctness'].shape)\n",
    "print(c_data['harness_hendrycksTest_abstract_algebra_5']['correctness'].shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14f180-d322-4cd5-8d0c-4ae4fef04127",
   "metadata": {},
   "source": [
    "In this dataset, we have data from 395 models. Let's see the names of some of them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6c4201-0675-42e5-8a7a-8cf75592e661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395,\n",
       " ['open-llm-leaderboard/details_zhengr__MixTAO-7Bx2-MoE-DPO',\n",
       "  'open-llm-leaderboard/details_alignment-handbook__zephyr-7b-sft-full',\n",
       "  'open-llm-leaderboard/details_rombodawg__Leaderboard-killer-MoE_4x7b',\n",
       "  'open-llm-leaderboard/details_FelixChao__ExtremeDolphin-MoE',\n",
       "  'open-llm-leaderboard/details_LoSboccacc__orthogonal-2x7B-base',\n",
       "  'open-llm-leaderboard/details_moreh__MoMo-70B-lora-1.8.6-DPO',\n",
       "  'open-llm-leaderboard/details_deepseek-ai__deepseek-moe-16b-base',\n",
       "  'open-llm-leaderboard/details_Swisslex__Mixtral-Orca-v0.1',\n",
       "  'open-llm-leaderboard/details_wang7776__Mistral-7B-Instruct-v0.2-sparsity-20',\n",
       "  'open-llm-leaderboard/details_nfaheem__Marcoroni-7b-DPO-Merge'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['models']),data['models'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8135b-e9ec-468a-85a7-3cd3fc3d31fe",
   "metadata": {},
   "source": [
    "Below, we will process the data so all correctness scores (for all scenarios) are stored in $Y$. The dictionaries `scenarios_position` and `subscenarios_position` give the position of scenarios/subscenarios correctness scores in $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee09c25b-2dc4-4403-a972-9fb05cfe917b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395, 28659)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios_position, subscenarios_position = prepare_data(scenarios, data)\n",
    "Y = create_responses(scenarios, data)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002485a-1e82-409b-aaf2-ddb6a82bc315",
   "metadata": {},
   "source": [
    "For example, below you can see the scores for MMLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4dd9649-ba75-49c0-92fe-b00d2afc252e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1., ..., 1., 1., 0.],\n",
       "        [0., 0., 1., ..., 1., 1., 0.],\n",
       "        [0., 0., 1., ..., 1., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., ..., 1., 1., 0.],\n",
       "        [0., 0., 1., ..., 1., 1., 0.],\n",
       "        [1., 0., 1., ..., 1., 1., 0.]]),\n",
       " (395, 14042))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:,scenarios_position['mmlu']], Y[:,scenarios_position['mmlu']].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d062bc-1efe-4527-b40c-96fc295e05fe",
   "metadata": {},
   "source": [
    "For scenarios that have multiple subscenarios, it is usually the case that we want to give equal importance to individual subscenarios when computing the aggregated performance in that scenario. This is equivalent to using a weighted average when computing the aggregated performance. We will create balance_weights, a vector of weights to help us compute those weighted averages. These weights will be different than one only for MMLU, which is the only scenario with multiple subscenarios.\n",
    "\n",
    "We will use this when choosing the IRT dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c2e717-c575-4b0c-8067-15574c3dde3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_weights = np.ones(Y.shape[1])\n",
    "\n",
    "N = len(scenarios_position['mmlu'])\n",
    "n_sub = len(scenarios['mmlu'])\n",
    "for sub in scenarios['mmlu']:\n",
    "    n_i = len(subscenarios_position['mmlu'][sub])\n",
    "    balance_weights[subscenarios_position['mmlu'][sub]] = N/(n_sub*n_i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4dd3af-2d50-4ed5-bc36-09acf3f987fc",
   "metadata": {},
   "source": [
    "We can see below that first averaging within subscenarios and then computing a simple average is equivalent to using a weighted average from the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85903718-e30c-433b-b407-74fc9ebb05eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.322333605307685e-14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs1 = np.mean([Y[:,subscenarios_position['mmlu'][sub]].mean(axis=1) for sub in scenarios['mmlu']], axis=0)\n",
    "accs2 = (balance_weights*Y)[:,scenarios_position['mmlu']].mean(axis=1)\n",
    "\n",
    "np.abs(accs1 - accs2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106b620-7fe0-49bb-a8ac-3a946c15f751",
   "metadata": {},
   "source": [
    "## Obtaining estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c4412-ae69-4184-b106-191a1c151736",
   "metadata": {},
   "source": [
    "Let's split the data in train and test (recent models are placed in the test set). We will not used the training par in this notebook, since they were already used in `anchor_points.ipynb` and `training_irt.ipynb` to obtain anchor points and train the IRT model. We will not discretize $Y$ in the evaluation time, but that can be done if the user thinks it's needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9a448b-c1db-41f1-8965-b1fdde2b10a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y[:100]\n",
    "Y_train = Y[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c35a9-926c-430e-914e-c9ff726e1ed7",
   "metadata": {},
   "source": [
    "### Using anchor points to estimate performance in the test set and reporting the average prediction error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8510538-0287-428a-9a3b-fdb2b7be7ac5",
   "metadata": {},
   "source": [
    "Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a906aa1-dbb3-41f1-bbde-a2e18aefc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anchor.pickle', 'rb') as handle:\n",
    "    anchor = pickle.load(handle)\n",
    "\n",
    "anchor_points = anchor['anchor_points']\n",
    "anchor_weights = anchor['anchor_weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4445d4-c36a-4240-b907-9c8889cf249c",
   "metadata": {},
   "source": [
    "In practice, `Y_test` would be filled with NaNs except in the indices given `seen_items` below:\n",
    "\n",
    "`seen_items = np.hstack([np.array(scenarios_position[scenario])[anchor_points[scenario]] for scenario in scenarios.keys()]).tolist()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2164d4-e9ca-4040-9f59-e2140636ae1c",
   "metadata": {},
   "source": [
    "Computing estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a1fe3ab-a404-4947-9cbf-ac17239b05d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario: harness_truthfulqa_mc_0, avg. error: 0.016\n",
      "scenario: gsm8k, avg. error: 0.019\n",
      "scenario: winogrande, avg. error: 0.024\n",
      "scenario: arc, avg. error: 0.023\n",
      "scenario: hellaswag, avg. error: 0.020\n",
      "scenario: mmlu, avg. error: 0.028\n"
     ]
    }
   ],
   "source": [
    "preds = {}\n",
    "for scenario in scenarios.keys():\n",
    "    Y_anchor = Y_test[:,scenarios_position[scenario]][:,anchor_points[scenario]]\n",
    "    preds[scenario] = (Y_anchor*anchor_weights[scenario]).sum(axis=1) # Predictions\n",
    "    true = (balance_weights*Y_test)[:,scenarios_position[scenario]].mean(axis=1) # True performance\n",
    "\n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(preds[scenario]-true).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640d18f-066f-4825-9770-3db5584af4d9",
   "metadata": {},
   "source": [
    "### Combining anchor points with IRT to estimate performance in the test set and reporting the average prediction error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ca27c-1105-4204-a01a-a98785bcacef",
   "metadata": {},
   "source": [
    "Loading IRT parameter estimates and recording all seen examples indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72850d84-42bf-4eae-ab70-ac7facb49be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B, _ = load_irt_parameters('data/irt_model/')\n",
    "seen_items = np.hstack([np.array(scenarios_position[scenario])[anchor_points[scenario]] for scenario in scenarios.keys()]).tolist()\n",
    "unseen_items = [i for i in range(Y_train.shape[1]) if i not in seen_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab64f238-4277-49ef-b09f-4bdc26e0ebe6",
   "metadata": {},
   "source": [
    "Estimating ability parameters for test LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7757ecd-d79d-413e-a80c-5519e7a1ebee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 95/95 [00:03<00:00, 25.97it/s]\n"
     ]
    }
   ],
   "source": [
    "thetas = [estimate_ability_parameters(Y_test[j][seen_items], A[:, :, seen_items], B[:, :, seen_items]) for j in tqdm(range(Y_test.shape[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9225ff-b94b-4e22-b212-8ecda77074c2",
   "metadata": {},
   "source": [
    "#### p-IRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7129272a-808b-41ef-a31b-948236ff1d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario: harness_truthfulqa_mc_0, avg. error: 0.018\n",
      "scenario: gsm8k, avg. error: 0.023\n",
      "scenario: winogrande, avg. error: 0.015\n",
      "scenario: arc, avg. error: 0.012\n",
      "scenario: hellaswag, avg. error: 0.016\n",
      "scenario: mmlu, avg. error: 0.025\n"
     ]
    }
   ],
   "source": [
    "pirt_preds = {}\n",
    "for scenario in scenarios.keys():\n",
    "\n",
    "    ind_seen = [u for u in seen_items if u in scenarios_position[scenario]]\n",
    "    ind_unseen = [u for u in unseen_items if u in scenarios_position[scenario]]\n",
    "    pirt_lambd = Y_anchor.shape[1]/len(scenarios_position[scenario])\n",
    "\n",
    "    pirt_pred = []\n",
    "    \n",
    "    for j in range(Y_test.shape[0]):\n",
    "        data_part = (balance_weights*Y_test)[j,ind_seen].mean()\n",
    "        irt_part = (balance_weights*item_curve(thetas[j], A, B))[0,ind_unseen].mean()\n",
    "        pirt_pred.append(pirt_lambd*data_part + (1-pirt_lambd)*irt_part) \n",
    "        \n",
    "    pirt_preds[scenario] = np.array(pirt_pred) # Predictions\n",
    "    true = (balance_weights*Y_test)[:,scenarios_position[scenario]].mean(axis=1) # True performance\n",
    "    \n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(pirt_preds[scenario]-true).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35abee45-0bfe-45a0-86db-78250587ffb5",
   "metadata": {},
   "source": [
    "#### gp-IRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b626f-ae61-4f87-a442-36f93fb6f224",
   "metadata": {},
   "source": [
    "Loading lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25bb4c62-b38f-41ba-aa07-6279f410674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lambds.pickle', 'rb') as handle:\n",
    "    lambds = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3343aa-54f2-40cd-8620-97d9b219d65a",
   "metadata": {},
   "source": [
    "Computing estimates and their average errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5079eb8-7e85-473e-b7de-b4191881288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario: harness_truthfulqa_mc_0, avg. error: 0.013\n",
      "scenario: gsm8k, avg. error: 0.018\n",
      "scenario: winogrande, avg. error: 0.013\n",
      "scenario: arc, avg. error: 0.012\n",
      "scenario: hellaswag, avg. error: 0.014\n",
      "scenario: mmlu, avg. error: 0.023\n"
     ]
    }
   ],
   "source": [
    "gpirt_preds = {}\n",
    "for scenario in scenarios.keys():\n",
    "    gpirt_preds[scenario] = lambds[scenario]*preds[scenario]  + (1-lambds[scenario])*pirt_preds[scenario]\n",
    "    true = (balance_weights*Y_test)[:,scenarios_position[scenario]].mean(axis=1) # True performance\n",
    "    \n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(gpirt_preds[scenario]-true).mean():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
